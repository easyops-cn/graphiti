# 24. 实体去重流程优化 - 单轮 LLM 去重（2025-12-18）

## 问题背景

原有的 `add_episode_bulk` 批量导入流程中，实体去重经过**三轮**处理：

1. **第一轮（NodeResolutionsWithScores）**：在 `resolve_extracted_nodes` 中，对每个 episode 的节点与数据库候选进行 LLM 去重，使用 `nodes_with_scores` 提示词
2. **第二轮（确定性匹配）**：在 `dedupe_nodes_bulk` 中，对批次内节点进行精确字符串和 MinHash 匹配
3. **第三轮（NodeResolutions）**：在 `semantic_dedupe_nodes_bulk` 中，attributes 提取后对同类型节点进行语义去重

**问题分析**：

1. **第一轮 LLM 调用时 summary 为空**：此时还没有调用 `extract_attributes_from_nodes`，节点没有 summary 和 attributes，LLM 只能根据 name 判断
2. **第三轮才有完整信息**：此时节点已经有 summary 和 attributes，LLM 可以做出更准确的判断
3. **重复工作**：第一轮和第三轮都调用 LLM 做语义去重，存在重复

**性能分析**（trace c08ad7a370c37014d082d6a9a5d81bf9）：

- 第一轮 LLM 调用：12 次，耗时 280 秒（06:02:08 - 06:06:52）
- 第三轮 LLM 调用：16 次，耗时 40 秒（06:11:28 - 06:12:09）

**第一轮 `NodeResolutionsWithScores` 输出量大**：

每个 LLM 调用需要输出所有候选的相似度评分：
- 5 个节点 × 36 个候选 = 180 组评分数据
- 每组包含 `candidate_idx`, `similarity_score`, `is_same_entity`, `reasoning`

## 解决方案

**去掉第一轮 LLM 去重，只保留第三轮**：

1. `dedupe_nodes_bulk`：只做数据库搜索 + 确定性匹配，收集数据库候选
2. `_resolve_nodes_and_edges_bulk`：去掉 `resolve_extracted_nodes` 调用，只提取 attributes
3. `semantic_dedupe_nodes_bulk`：合并数据库候选和批次内候选，统一做 LLM 去重，**并行执行**

**优化后的流程**：

```
Episode 写入
    │
    ▼
┌─────────────────────────────────────┐
│ 1. extract_nodes_and_edges_bulk     │
│    - LLM 抽取实体和关系              │
└─────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────┐
│ 2. dedupe_nodes_bulk (优化后)        │
│    - 数据库搜索（已按 entity type 过滤）│
│    - 确定性匹配（精确字符串 + MinHash）│
│    - 返回 db_candidates_by_type      │
│    - **不调用 LLM**                  │
└─────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────┐
│ 3. _resolve_nodes_and_edges_bulk    │
│    - extract_attributes_from_nodes  │
│    - 获取 summary 和 attributes     │
│    - **不调用 resolve_extracted_nodes** │
└─────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────┐
│ 4. semantic_dedupe_nodes_bulk (优化后)│
│    - 合并候选：同批次 + 数据库同类型  │
│    - 使用 NodeResolutions（轻量输出）│
│    - **并行处理所有实体**            │
└─────────────────────────────────────┘
    │
    ▼
保存到数据库
```

## 关键优化点

1. **去掉 NodeResolutionsWithScores**：不再需要输出所有候选的评分，改用 NodeResolutions 只返回 duplicate_idx
2. **数据库候选复用**：`dedupe_nodes_bulk` 收集的候选传递给 `semantic_dedupe_nodes_bulk` 使用
3. **串行改并行**：`semantic_dedupe_nodes_bulk` 原来是串行处理每个实体，现在改为并行处理所有实体

## 性能对比

| 指标 | 优化前 | 优化后 |
|-----|-------|-------|
| LLM 去重调用次数 | 第一轮 12 次 + 第三轮 16 次 = 28 次 | 只有一轮，取决于实体数量 |
| LLM 输出格式 | NodeResolutionsWithScores（每候选都打分） | NodeResolutions（只返回 duplicate_idx） |
| 执行方式 | 第三轮串行 | 第三轮并行 |
| 数据库搜索 | 两次（第一轮 + 第三轮） | 一次（第二步收集，第四步复用） |

## 修改文件清单

| 文件 | 修改内容 |
|-----|---------|
| `graphiti_core/utils/bulk_utils.py` | `dedupe_nodes_bulk()` 返回 `db_candidates_by_type`，去掉 `resolve_extracted_nodes` 调用 |
| `graphiti_core/utils/bulk_utils.py` | `semantic_dedupe_nodes_bulk()` 接收 `db_candidates_by_type`，合并候选，并行处理 |
| `graphiti_core/graphiti.py` | `_extract_and_dedupe_nodes_bulk()` 返回 `db_candidates_by_type` |
| `graphiti_core/graphiti.py` | `_resolve_nodes_and_edges_bulk()` 去掉 `resolve_extracted_nodes` 调用 |
| `graphiti_core/graphiti.py` | `add_episode_bulk()` 传递 `db_candidates_by_type` 给 `semantic_dedupe_nodes_bulk()` |

## 接口变更

**`dedupe_nodes_bulk()` 返回值变更**：

```python
# 优化前
async def dedupe_nodes_bulk(...) -> tuple[dict[str, list[EntityNode]], dict[str, str]]:

# 优化后
async def dedupe_nodes_bulk(...) -> tuple[dict[str, list[EntityNode]], dict[str, str], dict[str, list[EntityNode]]]:
    # 第三个返回值: db_candidates_by_type
```

**`semantic_dedupe_nodes_bulk()` 参数变更**：

```python
# 优化前
async def semantic_dedupe_nodes_bulk(
    clients: GraphitiClients,
    nodes: list[EntityNode],
    entity_types: dict[str, type[BaseModel]] | None = None,
) -> list[EntityNode]:

# 优化后
async def semantic_dedupe_nodes_bulk(
    clients: GraphitiClients,
    nodes: list[EntityNode],
    entity_types: dict[str, type[BaseModel]] | None = None,
    db_candidates_by_type: dict[str, list[EntityNode]] | None = None,  # 新增
) -> tuple[list[EntityNode], dict[str, str]]:  # 返回 (nodes, uuid_map)
```

## 回滚说明（2025-12-18）

**此优化已回滚**，恢复了 `resolve_extracted_nodes` 调用。

**回滚原因**：为实现延迟去重策略（Delayed Deduplication），需要恢复 Graphiti 原生的 LLM 去重流程。

**延迟去重策略**：
1. **快速写入**：使用 `resolve_extracted_nodes` 进行实时 LLM 去重（确定性匹配 + LLM 语义判断）
2. **后台维护**：后续实现批量合并脚本，通过 Cypher 语句合并漏网的重复实体

**回滚内容**：
1. `_resolve_nodes_and_edges_bulk`：恢复 `resolve_extracted_nodes` 调用
2. `add_episode_bulk`：注释掉 `semantic_dedupe_nodes_bulk` 调用

**当前去重流程**：
```
Episode 写入
    │
    ▼
┌─────────────────────────────────────┐
│ 1. extract_nodes_and_edges_bulk     │
│    - LLM 抽取实体和关系              │
└─────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────┐
│ 2. dedupe_nodes_bulk                │
│    - 数据库搜索                      │
│    - 确定性匹配（精确字符串 + MinHash）│
└─────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────┐
│ 3. _resolve_nodes_and_edges_bulk    │
│    - **resolve_extracted_nodes**    │
│      (确定性 + LLM 语义去重)         │
│    - extract_attributes_from_nodes  │
└─────────────────────────────────────┘
    │
    ▼
保存到数据库
```

---
